@article{abadiDeepLearningDifferential2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  author = {Abadi, Martín and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  date = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1607.00133},
  url = {https://arxiv.org/abs/1607.00133},
  urldate = {2023-12-26},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  version = {2},
  keywords = {Cryptography and Security (cs.CR),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/sergiorivera/Zotero/storage/ZQ4WH5RW/Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf}
}

@article{arjovskyPrincipledMethodsTraining2017,
  title = {Towards {{Principled Methods}} for {{Training Generative Adversarial Networks}}},
  author = {Arjovsky, Martin and Bottou, Léon},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1701.04862},
  url = {https://arxiv.org/abs/1701.04862},
  urldate = {2023-12-23},
  abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
  version = {1},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/sergiorivera/Zotero/storage/UCVJPCYK/Towards Principled Methods for Training Generative Adversarial Networks.pdf}
}

@report{beaulieu-jonesPrivacypreservingGenerativeDeep2017,
  type = {preprint},
  title = {Privacy-Preserving Generative Deep Neural Networks Support Clinical Data Sharing},
  author = {Beaulieu-Jones, Brett K. and Wu, Zhiwei Steven and Williams, Chris and Lee, Ran and Bhavnani, Sanjeev P. and Byrd, James Brian and Greene, Casey S.},
  date = {2017-07-05},
  institution = {{Bioinformatics}},
  doi = {10.1101/159756},
  url = {http://biorxiv.org/lookup/doi/10.1101/159756},
  urldate = {2023-12-23},
  abstract = {Abstract                        Background             Data sharing accelerates scientific progress but sharing individual level data while preserving patient privacy presents a barrier.                                   Methods and Results             Using pairs of deep neural networks, we generated simulated, synthetic “participants” that closely resemble participants of the SPRINT trial. We showed that such paired networks can be trained with differential privacy, a formal privacy framework that limits the likelihood that queries of the synthetic participants’ data could identify a real a participant in the trial. Machine-learning predictors built on the synthetic population generalize to the original dataset. This finding suggests that the synthetic data can be shared with others, enabling them to perform hypothesis-generating analyses as though they had the original trial data.                                   Conclusions             Deep neural networks that generate synthetic participants facilitate secondary analyses and reproducible investigation of clinical datasets by enhancing data sharing while preserving participant privacy.},
  langid = {english},
  file = {/Users/sergiorivera/Zotero/storage/KK7NIRFH/005122r2_supplemental_material.pdf;/Users/sergiorivera/Zotero/storage/YR8GDCZS/Beaulieu-Jones et al. - 2017 - Privacy-preserving generative deep neural networks.pdf}
}

@article{chenDistillingPortableGenerative2020,
  title = {Distilling {{Portable Generative Adversarial Networks}} for {{Image Translation}}},
  author = {Chen, Hanting and Wang, Yunhe and Shu, Han and Wen, Changyuan and Xu, Chunjing and Shi, Boxin and Xu, Chao and Xu, Chang},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {04},
  pages = {3585--3592},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i04.5765},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5765},
  urldate = {2024-01-02},
  abstract = {Despite Generative Adversarial Networks (GANs) have been widely used in various image-to-image translation tasks, they can be hardly applied on mobile devices due to their heavy computation and storage cost. Traditional network compression methods focus on visually recognition tasks, but never deal with generation tasks. Inspired by knowledge distillation, a student generator of fewer parameters is trained by inheriting the low-level and high-level information from the original heavy teacher generator. To promote the capability of student generator, we include a student discriminator to measure the distances between real images, and images generated by student and teacher generators. An adversarial learning process is therefore established to optimize student generator and student discriminator. Qualitative and quantitative analysis by conducting experiments on benchmark datasets demonstrate that the proposed method can learn portable generative models with strong performance.},
  file = {/Users/sergiorivera/Zotero/storage/IYKCFKJR/Chen et al. - 2020 - Distilling Portable Generative Adversarial Network.pdf}
}

@article{choiGeneratingMultilabelDiscrete2017,
  title = {Generating {{Multi-label Discrete Patient Records}} Using {{Generative Adversarial Networks}}},
  author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1703.06490},
  url = {https://arxiv.org/abs/1703.06490},
  urldate = {2024-01-02},
  abstract = {Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.},
  version = {3},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Neural and Evolutionary Computing (cs.NE)},
  file = {/Users/sergiorivera/Zotero/storage/QIICBN34/Choi et al. - 2017 - Generating Multi-label Discrete Patient Records us.pdf}
}

@article{deoliveiraLearningParticlePhysics2017,
  title = {Learning {{Particle Physics}} by {{Example}}: {{Location-Aware Generative Adversarial Networks}} for {{Physics Synthesis}}},
  shorttitle = {Learning {{Particle Physics}} by {{Example}}},
  author = {family=Oliveira, given=Luke, prefix=de, useprefix=true and Paganini, Michela and Nachman, Benjamin},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1701.05927},
  url = {https://arxiv.org/abs/1701.05927},
  urldate = {2024-01-02},
  abstract = {We provide a bridge between generative modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel Generative Adversarial Network (GAN) architecture to the production of jet images -- 2D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware Generative Adversarial Network, that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of GAN-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i.e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of GAN-produced simulations of the natural world. This work provides a base for further explorations of GANs for use in faster simulation in High Energy Particle Physics.},
  version = {2},
  keywords = {{Data Analysis, Statistics and Probability (physics.data-an)},FOS: Computer and information sciences,FOS: Physical sciences,High Energy Physics - Experiment (hep-ex),Machine Learning (stat.ML)},
  file = {/Users/sergiorivera/Zotero/storage/HCIVPQAC/de Oliveira et al. - 2017 - Learning Particle Physics by Example Location-Awa.pdf}
}

@article{dworkAlgorithmicFoundationsDifferential2013,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  date = {2013},
  journaltitle = {Foundations and Trends® in Theoretical Computer Science},
  shortjournal = {FNT in Theoretical Computer Science},
  volume = {9},
  number = {3-4},
  pages = {211--407},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-042},
  urldate = {2023-12-26},
  langid = {english},
  file = {/Users/sergiorivera/Zotero/storage/XAQUKPIE/Dwork and Roth - 2013 - The Algorithmic Foundations of Differential Privac.pdf}
}

@article{estebanRealvaluedMedicalTime2017,
  title = {Real-Valued ({{Medical}}) {{Time Series Generation}} with {{Recurrent Conditional GANs}}},
  author = {Esteban, Cristóbal and Hyland, Stephanie L. and Rätsch, Gunnar},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1706.02633},
  url = {https://arxiv.org/abs/1706.02633},
  urldate = {2024-01-02},
  abstract = {Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from 'serialised' MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data.},
  version = {2},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/sergiorivera/Zotero/storage/I9CAXFPU/Esteban et al. - 2017 - Real-valued (Medical) Time Series Generation with .pdf}
}

@inproceedings{frid-adarSyntheticDataAugmentation2018,
  title = {Synthetic Data Augmentation Using {{GAN}} for Improved Liver Lesion Classification},
  booktitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  author = {Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
  date = {2018-04},
  pages = {289--293},
  publisher = {{IEEE}},
  location = {{Washington, DC}},
  doi = {10.1109/ISBI.2018.8363576},
  url = {https://ieeexplore.ieee.org/document/8363576/},
  urldate = {2024-01-12},
  eventtitle = {2018 {{IEEE}} 15th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}} 2018)},
  isbn = {978-1-5386-3636-7},
  file = {/Users/sergiorivera/Zotero/storage/BX56BXNC/Frid-Adar et al. - 2018 - Synthetic data augmentation using GAN for improved.pdf}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}} (2014)},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1406.2661},
  url = {https://arxiv.org/abs/1406.2661},
  urldate = {2023-12-23},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  version = {1},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {/Users/sergiorivera/Zotero/storage/LASA6JIZ/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf}
}

@article{goodfellowGenerativeAdversarialNetworks2020,
  title = {Generative {{Adversarial Networks}} (2018)},
  author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2020-10-22},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {63},
  number = {11},
  pages = {139--144},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3422622},
  url = {https://dl.acm.org/doi/10.1145/3422622},
  urldate = {2023-12-23},
  abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the               generative modeling               problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
  langid = {english},
  file = {/Users/sergiorivera/Zotero/storage/HUQFTZ8J/Goodfellow et al. - 2020 - Generative adversarial networks.pdf}
}

@article{gopiNumericalCompositionDifferential2021,
  title = {Numerical {{Composition}} of {{Differential Privacy}}},
  author = {Gopi, Sivakanth and Lee, Yin Tat and Wutschitz, Lukas},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2106.02848},
  url = {https://arxiv.org/abs/2106.02848},
  urldate = {2024-01-12},
  abstract = {We give a fast algorithm to optimally compose privacy guarantees of differentially private (DP) algorithms to arbitrary accuracy. Our method is based on the notion of privacy loss random variables to quantify the privacy loss of DP algorithms. The running time and memory needed for our algorithm to approximate the privacy curve of a DP algorithm composed with itself \$k\$ times is \$\textbackslash tilde\{O\}(\textbackslash sqrt\{k\})\$. This improves over the best prior method by Koskela et al. (2020) which requires \$\textbackslash tildeΩ(k\^\{1.5\})\$ running time. We demonstrate the utility of our algorithm by accurately computing the privacy loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm speeds up the privacy computations by a few orders of magnitude compared to prior work, while maintaining similar accuracy.},
  version = {3},
  keywords = {Cryptography and Security (cs.CR),Data Structures and Algorithms (cs.DS),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/sergiorivera/Zotero/storage/7JXUTMMW/Gopi et al. - 2021 - Numerical Composition of Differential Privacy.pdf}
}

@inproceedings{haradalBiosignalDataAugmentation2018,
  title = {Biosignal {{Data Augmentation Based}} on {{Generative Adversarial Networks}}},
  booktitle = {2018 40th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Haradal, Shota and Hayashi, Hideaki and Uchida, Seiichi},
  date = {2018-07},
  pages = {368--371},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/EMBC.2018.8512396},
  url = {https://ieeexplore.ieee.org/document/8512396/},
  urldate = {2024-01-12},
  eventtitle = {2018 40th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  isbn = {978-1-5386-3646-6}
}

@article{heAMCAutoMLModel2018,
  title = {{{AMC}}: {{AutoML}} for {{Model Compression}} and {{Acceleration}} on {{Mobile Devices}}},
  shorttitle = {{{AMC}}},
  author = {He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  date = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1802.03494},
  url = {https://arxiv.org/abs/1802.03494},
  urldate = {2024-01-02},
  abstract = {Model compression is a critical technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted heuristics and rule-based policies that require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model compression policy. This learning-based compression policy outperforms conventional rule-based compression policy by having higher compression ratio, better preserving the accuracy and freeing human labor. Under 4x FLOPs reduction, we achieved 2.7\% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet and achieved 1.81x speedup of measured inference latency on an Android phone and 1.43x speedup on the Titan XP GPU, with only 0.1\% loss of ImageNet Top-1 accuracy.},
  version = {4},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/sergiorivera/Zotero/storage/CKVJVDAH/He et al. - 2018 - AMC AutoML for Model Compression and Acceleration.pdf}
}

@article{hosenHiMFRHybridMasked2022,
  title = {{{HiMFR}}: {{A Hybrid Masked Face Recognition Through Face Inpainting}}},
  shorttitle = {{{HiMFR}}},
  author = {Hosen, Md Imran and Islam, Md Baharul},
  date = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2209.08930},
  url = {https://arxiv.org/abs/2209.08930},
  urldate = {2024-01-12},
  abstract = {To recognize the masked face, one of the possible solutions could be to restore the occluded part of the face first and then apply the face recognition method. Inspired by the recent image inpainting methods, we propose an end-to-end hybrid masked face recognition system, namely HiMFR, consisting of three significant parts: masked face detector, face inpainting, and face recognition. The masked face detector module applies a pretrained Vision Transformer (ViT\textbackslash\_b32) to detect whether faces are covered with masked or not. The inpainting module uses a fine-tune image inpainting model based on a Generative Adversarial Network (GAN) to restore faces. Finally, the hybrid face recognition module based on ViT with an EfficientNetB3 backbone recognizes the faces. We have implemented and evaluated our proposed method on four different publicly available datasets: CelebA, SSDMNV2, MAFA, \{Pubfig83\} with our locally collected small dataset, namely Face5. Comprehensive experimental results show the efficacy of the proposed HiMFR method with competitive performance. Code is available at https://github.com/mdhosen/HiMFR},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/sergiorivera/Zotero/storage/GVYQG9ET/Hosen and Islam - 2022 - HiMFR A Hybrid Masked Face Recognition Through Fa.pdf}
}

@article{johnsonPerceptualLossesRealTime2016,
  title = {Perceptual {{Losses}} for {{Real-Time Style Transfer}} and {{Super-Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  date = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1603.08155},
  url = {https://arxiv.org/abs/1603.08155},
  urldate = {2024-01-12},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \textbackslash emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \textbackslash emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/sergiorivera/Zotero/storage/R3WGGLEG/Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf}
}

@inproceedings{karrasStyleBasedGeneratorArchitecture2019,
  title = {A {{Style-Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  date = {2019-06},
  pages = {4396--4405},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00453},
  url = {https://ieeexplore.ieee.org/document/8953766/},
  urldate = {2024-01-02},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  file = {/Users/sergiorivera/Zotero/storage/QA7SLMV8/Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf}
}

@article{ledigPhotoRealisticSingleImage2016a,
  title = {Photo-{{Realistic Single Image Super-Resolution Using}} a {{Generative Adversarial Network}}},
  author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
  date = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1609.04802},
  url = {https://arxiv.org/abs/1609.04802},
  urldate = {2024-01-02},
  abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
  version = {5},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (stat.ML)},
  file = {/Users/sergiorivera/Zotero/storage/KKYWEQ5P/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Usin.pdf}
}

@article{liuAdaDeepUsageDrivenAutomated2020,
  title = {{{AdaDeep}}: {{A Usage-Driven}}, {{Automated Deep Model Compression Framework}} for {{Enabling Ubiquitous Intelligent Mobiles}}},
  shorttitle = {{{AdaDeep}}},
  author = {Liu, Sicong and Du, Junzhao and Nan, Kaiming and {ZimuZhou} and Wang, Atlas and Lin, Yingyan},
  date = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2006.04432},
  url = {https://arxiv.org/abs/2006.04432},
  urldate = {2024-01-02},
  abstract = {Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a tremendously growing demand for bringing DNN-powered intelligence into mobile platforms. While the potential of deploying DNNs on resource-constrained platforms has been demonstrated by DNN compression techniques, the current practice suffers from two limitations: 1) merely stand-alone compression schemes are investigated even though each compression technique only suit for certain types of DNN layers; and 2) mostly compression techniques are optimized for DNNs' inference accuracy, without explicitly considering other application-driven system performance (e.g., latency and energy cost) and the varying resource availability across platforms (e.g., storage and processing capability). To this end, we propose AdaDeep, a usage-driven, automated DNN compression framework for systematically exploring the desired trade-off between performance and resource constraints, from a holistic system level. Specifically, in a layer-wise manner, AdaDeep automatically selects the most suitable combination of compression techniques and the corresponding compression hyperparameters for a given DNN. Thorough evaluations on six datasets and across twelve devices demonstrate that AdaDeep can achieve up to \$18.6\textbackslash times\$ latency reduction, \$9.8\textbackslash times\$ energy-efficiency improvement, and \$37.3\textbackslash times\$ storage reduction in DNNs while incurring negligible accuracy loss. Furthermore, AdaDeep also uncovers multiple novel combinations of compression techniques.},
  version = {1},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/sergiorivera/Zotero/storage/CR6IPEHS/Liu et al. - 2020 - AdaDeep A Usage-Driven, Automated Deep Model Comp.pdf}
}

@article{liuDeepLearningFace2014,
  title = {Deep {{Learning Face Attributes}} in the {{Wild}}},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  date = {2014},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1411.7766},
  url = {https://arxiv.org/abs/1411.7766},
  urldate = {2024-01-12},
  abstract = {Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.},
  version = {3},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/sergiorivera/Zotero/storage/RJGCCCVC/Liu et al. - 2014 - Deep Learning Face Attributes in the Wild.pdf}
}

@article{liuLearningEfficientConvolutional2017,
  title = {Learning {{Efficient Convolutional Networks}} through {{Network Slimming}}},
  author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1708.06519},
  url = {https://arxiv.org/abs/1708.06519},
  urldate = {2024-01-02},
  abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/sergiorivera/Zotero/storage/X2FEY6XL/Liu et al. - 2017 - Learning Efficient Convolutional Networks through .pdf}
}

@article{radfordUnsupervisedRepresentationLearning2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2015},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1511.06434},
  url = {https://arxiv.org/abs/1511.06434},
  urldate = {2024-01-12},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  version = {2},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/sergiorivera/Zotero/storage/V4FRTJCQ/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf}
}

@article{shuCoEvolutionaryCompressionUnpaired2019,
  title = {Co-{{Evolutionary Compression}} for {{Unpaired Image Translation}}},
  author = {Shu, Han and Wang, Yunhe and Jia, Xu and Han, Kai and Chen, Hanting and Xu, Chunjing and Tian, Qi and Xu, Chang},
  date = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1907.10804},
  url = {https://arxiv.org/abs/1907.10804},
  urldate = {2024-01-09},
  abstract = {Generative adversarial networks (GANs) have been successfully used for considerable computer vision tasks, especially the image-to-image translation. However, generators in these networks are of complicated architectures with large number of parameters and huge computational complexities. Existing methods are mainly designed for compressing and speeding-up deep neural networks in the classification task, and cannot be directly applied on GANs for image translation, due to their different objectives and training procedures. To this end, we develop a novel co-evolutionary approach for reducing their memory usage and FLOPs simultaneously. In practice, generators for two image domains are encoded as two populations and synergistically optimized for investigating the most important convolution filters iteratively. Fitness of each individual is calculated using the number of parameters, a discriminator-aware regularization, and the cycle consistency. Extensive experiments conducted on benchmark datasets demonstrate the effectiveness of the proposed method for obtaining compact and effective generators.},
  version = {1},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences,{FOS: Electrical engineering, electronic engineering, information engineering},Image and Video Processing (eess.IV),Machine Learning (cs.LG)}
}

@article{simonyanVeryDeepConvolutional2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1409.1556},
  url = {https://arxiv.org/abs/1409.1556},
  urldate = {2024-01-12},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  version = {6},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),FOS: Computer and information sciences},
  file = {/Users/sergiorivera/Zotero/storage/W6CZ4P3E/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}

@inproceedings{varkarakisDatasetCleaningCross2020,
  title = {Dataset {{Cleaning}} — {{A Cross Validation Methodology}} for {{Large Facial Datasets}} Using {{Face Recognition}}},
  booktitle = {2020 {{Twelfth International Conference}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author = {Varkarakis, Viktor and Corcoran, Peter},
  date = {2020-05},
  pages = {1--6},
  publisher = {{IEEE}},
  location = {{Athlone, Ireland}},
  doi = {10.1109/QoMEX48832.2020.9123123},
  url = {https://ieeexplore.ieee.org/document/9123123/},
  urldate = {2024-01-12},
  eventtitle = {2020 {{Twelfth International Conference}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  isbn = {978-1-72815-965-2},
  file = {/Users/sergiorivera/Zotero/storage/WSPEXIZ4/Varkarakis and Corcoran - 2020 - Dataset Cleaning — A Cross Validation Methodology .pdf}
}

@incollection{wangGANSlimmingAllinOne2020,
  title = {{{GAN Slimming}}: {{All-in-One GAN Compression}} by a {{Unified Optimization Framework}}},
  shorttitle = {{{GAN Slimming}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2020},
  author = {Wang, Haotao and Gui, Shupeng and Yang, Haichuan and Liu, Ji and Wang, Zhangyang},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  date = {2020},
  volume = {12349},
  pages = {54--73},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-58548-8_4},
  url = {https://link.springer.com/10.1007/978-3-030-58548-8_4},
  urldate = {2023-12-23},
  isbn = {978-3-030-58547-1 978-3-030-58548-8},
  langid = {english},
  file = {/Users/sergiorivera/Zotero/storage/42MZ8MTH/Wang et al. - 2020 - GAN Slimming All-in-One GAN Compression by a Unif.pdf}
}

@article{yousefpourOpacusUserFriendlyDifferential2021,
  title = {Opacus: {{User-Friendly Differential Privacy Library}} in {{PyTorch}}},
  shorttitle = {Opacus},
  author = {Yousefpour, Ashkan and Shilov, Igor and Sablayrolles, Alexandre and Testuggine, Davide and Prasad, Karthik and Malek, Mani and Nguyen, John and Ghosh, Sayan and Bharadwaj, Akash and Zhao, Jessica and Cormode, Graham and Mironov, Ilya},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2109.12298},
  url = {https://arxiv.org/abs/2109.12298},
  urldate = {2024-01-12},
  abstract = {We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional "micro batch" approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.},
  version = {4},
  keywords = {Cryptography and Security (cs.CR),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/sergiorivera/Zotero/storage/XPKKKZEI/Yousefpour et al. - 2021 - Opacus User-Friendly Differential Privacy Library.pdf}
}

@online{zhuUnpairedImagetoImageTranslation2020,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2020-08-24},
  eprint = {1703.10593},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.10593},
  urldate = {2024-01-02},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/sergiorivera/Zotero/storage/Z46GVJXY/Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf;/Users/sergiorivera/Zotero/storage/VYUDLRUQ/1703.html}
}
